{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659dd40-b36a-4070-b2aa-b5c67d733d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_query_obs_files(query_files, obs_files):\n",
    "    \"\"\"\n",
    "    Match query and observation files by their label.\n",
    "    \n",
    "    Args:\n",
    "        query_files: List of query files\n",
    "        obs_files: List of observation files\n",
    "    \n",
    "    Returns:\n",
    "        List of matched (query_file, obs_file) pairs\n",
    "    \"\"\"\n",
    "    # If the lengths match, assume they're already in the correct order\n",
    "    if len(query_files) == len(obs_files):\n",
    "        return list(zip(query_files, obs_files))\n",
    "    \n",
    "    # Try to match files by label\n",
    "    matched_files = []\n",
    "    for query_file in query_files:\n",
    "        # Extract label from filename (e.g., \"queries5c.txt\" -> \"5c\")\n",
    "        label_match = re.search(r'queries([^.]+)\\.txt', query_file)\n",
    "        if label_match:\n",
    "            label = label_match.group(1)\n",
    "            matching_obs = f\"observations{label}.txt\"\n",
    "            \n",
    "            if matching_obs in obs_files:\n",
    "                matched_files.append((query_file, matching_obs))\n",
    "    \n",
    "    if matched_files:\n",
    "        print(f\"Successfully matched {len(matched_files)} pairs of files by label\")\n",
    "        return matched_files\n",
    "    \n",
    "    print(\"Could not match files by label. Using the files in order.\")\n",
    "    # Take the minimum length to avoid index errors\n",
    "    min_len = min(len(query_files), len(obs_files))\n",
    "    return list(zip(query_files[:min_len], obs_files[:min_len]))import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_array_string(array_str):\n",
    "    \"\"\"Parse string representation of numpy array into actual numpy array.\"\"\"\n",
    "    # Remove 'array(' and trailing ')'\n",
    "    content = array_str.strip()\n",
    "    if content.startswith('array('):\n",
    "        content = content[6:-1]\n",
    "    elif content.startswith('np.float64('):\n",
    "        # For observations file with np.float64() format\n",
    "        content = content[11:-1]\n",
    "        return np.float64(float(content))\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(content))\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing array string: {array_str}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_initial_data(base_dirs):\n",
    "    \"\"\"\n",
    "    Load initial input and output data from function_n directories.\n",
    "    \n",
    "    Args:\n",
    "        base_dirs: List of directories containing function_n subdirectories\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with function inputs and outputs\n",
    "    \"\"\"\n",
    "    functions_data = {f: {'inputs': [], 'outputs': []} for f in range(1, 9)}\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        for func_idx in range(1, 9):\n",
    "            func_dir = os.path.join(base_dir, f\"function_{func_idx}\")\n",
    "            \n",
    "            if os.path.exists(func_dir):\n",
    "                input_file = os.path.join(func_dir, \"initial_inputs.npy\")\n",
    "                output_file = os.path.join(func_dir, \"initial_outputs.npy\")\n",
    "                \n",
    "                if os.path.exists(input_file):\n",
    "                    inputs = np.load(input_file)\n",
    "                    functions_data[func_idx]['inputs'].append(inputs)\n",
    "                \n",
    "                if os.path.exists(output_file):\n",
    "                    outputs = np.load(output_file)\n",
    "                    functions_data[func_idx]['outputs'].append(outputs)\n",
    "    \n",
    "    return functions_data\n",
    "\n",
    "def load_trial_data(query_files, observation_files):\n",
    "    \"\"\"\n",
    "    Load trial data from queries and observations text files.\n",
    "    \n",
    "    Args:\n",
    "        query_files: List of query files (queriesXX.txt)\n",
    "        observation_files: List of observation files (observationsXX.txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with function inputs and outputs\n",
    "    \"\"\"\n",
    "    functions_data = {f: {'inputs': [], 'outputs': []} for f in range(1, 9)}\n",
    "    \n",
    "    # Process each pair of query and observation files\n",
    "    for query_file, obs_file in zip(query_files, observation_files):\n",
    "        print(f\"Processing {query_file} and {obs_file}...\")\n",
    "        \n",
    "        # Read and parse query file\n",
    "        with open(query_file, 'r') as f:\n",
    "            query_content = f.read()\n",
    "        \n",
    "        # Read and parse observation file\n",
    "        with open(obs_file, 'r') as f:\n",
    "            obs_content = f.read()\n",
    "        \n",
    "        # Split content into trials (each line is a trial with data for all 8 functions)\n",
    "        query_lines = [line.strip() for line in query_content.split('\\n') if line.strip()]\n",
    "        obs_lines = [line.strip() for line in obs_content.split('\\n') if line.strip()]\n",
    "        \n",
    "        # Make sure we have matching lines\n",
    "        assert len(query_lines) == len(obs_lines), f\"Mismatch in lines between {query_file} and {obs_file}\"\n",
    "        \n",
    "        # Process each line (trial)\n",
    "        for trial_idx, (query_line, obs_line) in enumerate(zip(query_lines, obs_lines)):\n",
    "            print(f\"  Processing trial {trial_idx+1}/{len(query_lines)}\")\n",
    "            \n",
    "            # Each line contains a list of arrays for each function\n",
    "            # Format: [array([...]), array([...]), ..., array([...])]\n",
    "            # First extract the outer list\n",
    "            query_line = query_line.strip()\n",
    "            obs_line = obs_line.strip()\n",
    "            \n",
    "            if not (query_line.startswith('[') and query_line.endswith(']')):\n",
    "                print(f\"  Warning: Query line doesn't have expected format: {query_line[:50]}...\")\n",
    "                continue\n",
    "                \n",
    "            if not (obs_line.startswith('[') and obs_line.endswith(']')):\n",
    "                print(f\"  Warning: Observation line doesn't have expected format: {obs_line[:50]}...\")\n",
    "                continue\n",
    "            \n",
    "            # Extract each array item\n",
    "            # This regex finds all array(...) patterns or np.float64(...) patterns\n",
    "            query_arrays = re.findall(r'array\\(\\[[^\\]]*\\]\\)', query_line)\n",
    "            obs_arrays = re.findall(r'np\\.float64\\([^)]*\\)', obs_line)\n",
    "            \n",
    "            # Make sure we have 8 arrays (one for each function)\n",
    "            if len(query_arrays) != 8:\n",
    "                print(f\"  Warning: Expected 8 arrays, but found {len(query_arrays)} in trial {trial_idx+1}\")\n",
    "                continue\n",
    "                \n",
    "            # Make sure we have 8 values (one for each function)\n",
    "            if len(obs_arrays) != 8:\n",
    "                print(f\"  Warning: Expected 8 values, but found {len(obs_arrays)} in trial {trial_idx+1}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse and store each function's input/output\n",
    "            for func_idx in range(1, 9):\n",
    "                func_input = parse_array_string(query_arrays[func_idx-1])\n",
    "                func_output = parse_array_string(obs_arrays[func_idx-1])\n",
    "                \n",
    "                if func_input is not None and func_output is not None:\n",
    "                    functions_data[func_idx]['inputs'].append(func_input)\n",
    "                    functions_data[func_idx]['outputs'].append(func_output)\n",
    "    \n",
    "    return functions_data\n",
    "\n",
    "def combine_data_in_order(data_list):\n",
    "    \"\"\"\n",
    "    Combine data in the specific order provided.\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of data dictionaries in the desired order\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with combined data preserving the order\n",
    "    \"\"\"\n",
    "    combined_data = {f: {'inputs': [], 'outputs': []} for f in range(1, 9)}\n",
    "    \n",
    "    # Process each data source in order\n",
    "    for data_source in data_list:\n",
    "        for func_idx in range(1, 9):\n",
    "            if func_idx in data_source:\n",
    "                if 'inputs' in data_source[func_idx] and data_source[func_idx]['inputs']:\n",
    "                    combined_data[func_idx]['inputs'].extend(data_source[func_idx]['inputs'])\n",
    "                if 'outputs' in data_source[func_idx] and data_source[func_idx]['outputs']:\n",
    "                    combined_data[func_idx]['outputs'].extend(data_source[func_idx]['outputs'])\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def save_combined_data(combined_data, output_dir):\n",
    "    \"\"\"\n",
    "    Save combined data to output .npy files.\n",
    "    \n",
    "    Args:\n",
    "        combined_data: Dictionary with combined data\n",
    "        output_dir: Directory to save the output files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for func_idx in range(1, 9):\n",
    "        # Convert lists to numpy arrays for saving\n",
    "        if combined_data[func_idx]['inputs']:\n",
    "            # Stack inputs if they're arrays, otherwise convert list to array\n",
    "            try:\n",
    "                # Handle case where inputs are arrays\n",
    "                inputs_array = np.vstack(combined_data[func_idx]['inputs'])\n",
    "            except ValueError:\n",
    "                # Handle case where inputs are mixed shapes\n",
    "                inputs_array = np.array(combined_data[func_idx]['inputs'], dtype=object)\n",
    "            \n",
    "            np.save(os.path.join(output_dir, f\"function_{func_idx}_inputs.npy\"), inputs_array)\n",
    "            print(f\"Saved combined inputs for function {func_idx} with shape {inputs_array.shape}\")\n",
    "        \n",
    "        if combined_data[func_idx]['outputs']:\n",
    "            try:\n",
    "                # Handle case where outputs are arrays or scalars\n",
    "                outputs_array = np.array(combined_data[func_idx]['outputs'])\n",
    "            except ValueError:\n",
    "                # Handle case where outputs are mixed shapes\n",
    "                outputs_array = np.array(combined_data[func_idx]['outputs'], dtype=object)\n",
    "            \n",
    "            np.save(os.path.join(output_dir, f\"function_{func_idx}_outputs.npy\"), outputs_array)\n",
    "            print(f\"Saved combined outputs for function {func_idx} with shape {outputs_array.shape}\")\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    import json\n",
    "    \n",
    "    # Create command-line parser\n",
    "    parser = argparse.ArgumentParser(description='Combine function data from multiple sources in a specific order.')\n",
    "    parser.add_argument('--config', help='JSON configuration file specifying the data sources and their order')\n",
    "    parser.add_argument('--output-dir', default='./combined_data', \n",
    "                        help='Directory to save combined data (default: ./combined_data)')\n",
    "    parser.add_argument('--verbose', '-v', action='store_true', \n",
    "                        help='Print verbose output')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.config:\n",
    "        # Load configuration from JSON file\n",
    "        with open(args.config, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    else:\n",
    "        # Use default configuration\n",
    "        config = {\n",
    "            \"sources\": [\n",
    "                {\"type\": \"initial\", \"dirs\": [\"./\"]},\n",
    "                {\"type\": \"trial\", \"query_pattern\": \"queries*.txt\", \"obs_pattern\": \"observations*.txt\"}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # Process each data source in the specified order\n",
    "    all_data_sources = []\n",
    "    \n",
    "    for i, source in enumerate(config[\"sources\"]):\n",
    "        source_type = source[\"type\"]\n",
    "        print(f\"\\nProcessing source {i+1}/{len(config['sources'])} (type: {source_type})...\")\n",
    "        \n",
    "        if source_type == \"initial\":\n",
    "            # Load data from function_n directories\n",
    "            dirs = source.get(\"dirs\", [\"./\"])\n",
    "            print(f\"Loading initial data from {dirs}...\")\n",
    "            data = load_initial_data(dirs)\n",
    "            all_data_sources.append(data)\n",
    "            \n",
    "        elif source_type == \"trial\":\n",
    "            # Load data from trial files\n",
    "            query_pattern = source.get(\"query_pattern\", \"queries*.txt\")\n",
    "            obs_pattern = source.get(\"obs_pattern\", \"observations*.txt\")\n",
    "            \n",
    "            # Find all query and observation files\n",
    "            query_files = sorted(glob.glob(query_pattern))\n",
    "            obs_files = sorted(glob.glob(obs_pattern))\n",
    "            \n",
    "            print(f\"Found {len(query_files)} query files and {len(obs_files)} observation files\")\n",
    "            \n",
    "            # Match query and observation files\n",
    "            matched_files = match_query_obs_files(query_files, obs_files)\n",
    "            \n",
    "            if matched_files:\n",
    "                query_files = [pair[0] for pair in matched_files]\n",
    "                obs_files = [pair[1] for pair in matched_files]\n",
    "                print(f\"Loading trial data from {len(query_files)} matched file pairs...\")\n",
    "                data = load_trial_data(query_files, obs_files)\n",
    "                all_data_sources.append(data)\n",
    "            else:\n",
    "                print(\"No matching query and observation files found.\")\n",
    "    \n",
    "    # Combine all data in the specified order\n",
    "    print(\"\\nCombining data in the specified order...\")\n",
    "    combined_data = combine_data_in_order(all_data_sources)\n",
    "    \n",
    "    # Print summary of combined data\n",
    "    print(\"\\nSummary of combined data:\")\n",
    "    for func_idx in range(1, 9):\n",
    "        num_inputs = len(combined_data[func_idx]['inputs'])\n",
    "        num_outputs = len(combined_data[func_idx]['outputs'])\n",
    "        print(f\"Function {func_idx}: {num_inputs} inputs, {num_outputs} outputs\")\n",
    "    \n",
    "    # Save combined data\n",
    "    print(f\"\\nSaving combined data to {args.output_dir}...\")\n",
    "    save_combined_data(combined_data, args.output_dir)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
